{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-08T13:02:01.148962Z",
     "iopub.status.busy": "2025-02-08T13:02:01.148652Z",
     "iopub.status.idle": "2025-02-08T13:02:01.266773Z",
     "shell.execute_reply": "2025-02-08T13:02:01.265996Z",
     "shell.execute_reply.started": "2025-02-08T13:02:01.148935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem        Size  Used Avail Use% Mounted on\n",
      "overlay           7.9T  6.0T  1.9T  77% /\n",
      "tmpfs              64M     0   64M   0% /dev\n",
      "shm                14G     0   14G   0% /dev/shm\n",
      "/dev/loop1         20G   68K   20G   1% /kaggle/lib\n",
      "/dev/sda1         122G  4.5G  118G   4% /opt/bin\n",
      "/dev/mapper/snap  7.9T  6.0T  1.9T  77% /etc/hosts\n",
      "tmpfs              16G     0   16G   0% /proc/acpi\n",
      "tmpfs              16G     0   16G   0% /proc/scsi\n",
      "tmpfs              16G     0   16G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h #Checks Storage Spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣ **Setup and Installing Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:02:10.920294Z",
     "iopub.status.busy": "2025-02-08T13:02:10.919946Z",
     "iopub.status.idle": "2025-02-08T13:03:43.712558Z",
     "shell.execute_reply": "2025-02-08T13:03:43.711485Z",
     "shell.execute_reply.started": "2025-02-08T13:02:10.920266Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
      "Collecting huggingsound\n",
      "  Downloading huggingsound-0.1.6-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.5.1+cu121)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting jiwer<3.0.0,>=2.5.1 (from huggingsound)\n",
      "  Downloading jiwer-2.6.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting librosa<0.10.0,>=0.9.2 (from huggingsound)\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingsound to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torch==2.6.0 (from torchaudio)\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0->torchaudio)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.5.0 (from torchaudio)\n",
      "  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting triton==3.1.0 (from torch==2.5.0->torchaudio)\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.4.1 (from torchaudio)\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1->torchaudio)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.1->torchaudio)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchaudio) (12.6.85)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.4.0 (from torchaudio)\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.3.1 (from torchaudio)\n",
      "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1->torchaudio)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting triton==2.3.1 (from torch==2.3.1->torchaudio)\n",
      "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.3.0 (from torchaudio)\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting triton==2.3.0 (from torch==2.3.0->torchaudio)\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingsound to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.2.2 (from torchaudio)\n",
      "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->torchaudio)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting triton==2.2.0 (from torch==2.2.2->torchaudio)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.2.1 (from torchaudio)\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.2.0 (from torchaudio)\n",
      "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.1.2 (from torchaudio)\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->torchaudio)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting triton==2.1.0 (from torch==2.1.2->torchaudio)\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.1.1 (from torchaudio)\n",
      "  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting torch==2.1.0 (from torchaudio)\n",
      "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.0.2-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting torch==2.0.1 (from torchaudio)\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->torchaudio)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1->torchaudio)\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchaudio) (75.1.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchaudio) (0.45.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchaudio) (3.31.2)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->torchaudio)\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting torch==2.0.0 (from torchaudio)\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting torch==1.13.1 (from torchaudio)\n",
      "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting torch==1.13.0 (from torchaudio)\n",
      "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting torch==1.12.1 (from torchaudio)\n",
      "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer<3.0.0,>=2.5.1->huggingsound) (8.1.7)\n",
      "Collecting rapidfuzz==2.13.7 (from jiwer<3.0.0,>=2.5.1->huggingsound)\n",
      "  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (4.4.2)\n",
      "Collecting resampy>=0.2.2 (from librosa<0.10.0,>=0.9.2->huggingsound)\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.10/dist-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.10.0,>=0.9.2->huggingsound) (1.8.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.45.1->librosa<0.10.0,>=0.9.2->huggingsound) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa<0.10.0,>=0.9.2->huggingsound) (4.3.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa<0.10.0,>=0.9.2->huggingsound) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.2->librosa<0.10.0,>=0.9.2->huggingsound) (1.17.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>=0.9.2->huggingsound) (2.22)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading torchaudio-0.12.1-cp310-cp310-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingsound-0.1.6-py3-none-any.whl (28 kB)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m490.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m449.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiwer-2.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, rapidfuzz, fsspec, ffmpeg-python, torchaudio, jiwer, resampy, librosa, datasets, huggingsound\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.5.1+cu121\n",
      "    Uninstalling torchaudio-2.5.1+cu121:\n",
      "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
      "  Attempting uninstall: librosa\n",
      "    Found existing installation: librosa 0.10.2.post1\n",
      "    Uninstalling librosa-0.10.2.post1:\n",
      "      Successfully uninstalled librosa-0.10.2.post1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.2.0\n",
      "    Uninstalling datasets-3.2.0:\n",
      "      Successfully uninstalled datasets-3.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\n",
      "peft 0.14.0 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\n",
      "pytorch-lightning 2.5.0.post0 requires torch>=2.1.0, but you have torch 1.12.1 which is incompatible.\n",
      "stable-baselines3 2.1.0 requires torch>=1.13, but you have torch 1.12.1 which is incompatible.\n",
      "torchmetrics 1.6.1 requires torch>=2.0.0, but you have torch 1.12.1 which is incompatible.\n",
      "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.21.0 ffmpeg-python-0.2.0 fsspec-2024.6.1 huggingsound-0.1.6 jiwer-2.6.0 librosa-0.9.2 rapidfuzz-2.13.7 resampy-0.4.3 torch-1.12.1 torchaudio-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg-python transformers datasets torchaudio huggingface_hub huggingsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:04:08.467918Z",
     "iopub.status.busy": "2025-02-08T13:04:08.467633Z",
     "iopub.status.idle": "2025-02-08T13:04:17.335554Z",
     "shell.execute_reply": "2025-02-08T13:04:17.334517Z",
     "shell.execute_reply.started": "2025-02-08T13:04:08.467893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: deep-translator\n",
      "Successfully installed deep-translator-1.11.4\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deep-translator\n",
    "!pip install sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:04:46.871881Z",
     "iopub.status.busy": "2025-02-08T13:04:46.871546Z",
     "iopub.status.idle": "2025-02-08T13:06:53.570548Z",
     "shell.execute_reply": "2025-02-08T13:06:53.569581Z",
     "shell.execute_reply.started": "2025-02-08T13:04:46.871851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.12.1\n",
      "Uninstalling torch-1.12.1:\n",
      "  Successfully uninstalled torch-1.12.1\n",
      "Found existing installation: torchvision 0.20.1+cu121\n",
      "Uninstalling torchvision-0.20.1+cu121:\n",
      "  Successfully uninstalled torchvision-0.20.1+cu121\n",
      "Found existing installation: torchaudio 0.12.1\n",
      "Uninstalling torchaudio-0.12.1:\n",
      "  Successfully uninstalled torchaudio-0.12.1\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (27 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl (848.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m994.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchaudio, torchvision\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\n",
      "huggingsound 0.1.6 requires torch!=1.12.0,<1.13.0,>=1.7, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# In case, there are CUDA issues, run this\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:06:57.302197Z",
     "iopub.status.busy": "2025-02-08T13:06:57.301865Z",
     "iopub.status.idle": "2025-02-08T13:07:06.013652Z",
     "shell.execute_reply": "2025-02-08T13:07:06.012619Z",
     "shell.execute_reply.started": "2025-02-08T13:06:57.302165Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yt-dlp\n",
      "  Downloading yt_dlp-2025.1.26-py3-none-any.whl.metadata (172 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yt_dlp-2025.1.26-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: yt-dlp\n",
      "Successfully installed yt-dlp-2025.1.26\n"
     ]
    }
   ],
   "source": [
    "!pip install yt-dlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2️⃣ **Import Required Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:09:53.017183Z",
     "iopub.status.busy": "2025-02-08T13:09:53.016875Z",
     "iopub.status.idle": "2025-02-08T13:09:53.021141Z",
     "shell.execute_reply": "2025-02-08T13:09:53.020254Z",
     "shell.execute_reply.started": "2025-02-08T13:09:53.017159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import ffmpeg\n",
    "import yt_dlp\n",
    "from transformers import pipeline\n",
    "from deep_translator import GoogleTranslator\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:07:40.744429Z",
     "iopub.status.busy": "2025-02-08T13:07:40.743767Z",
     "iopub.status.idle": "2025-02-08T13:07:40.750234Z",
     "shell.execute_reply": "2025-02-08T13:07:40.749284Z",
     "shell.execute_reply.started": "2025-02-08T13:07:40.744389Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:07:54.835374Z",
     "iopub.status.busy": "2025-02-08T13:07:54.835072Z",
     "iopub.status.idle": "2025-02-08T13:07:54.985625Z",
     "shell.execute_reply": "2025-02-08T13:07:54.984694Z",
     "shell.execute_reply.started": "2025-02-08T13:07:54.835350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mkdir downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:07:59.047324Z",
     "iopub.status.busy": "2025-02-08T13:07:59.047003Z",
     "iopub.status.idle": "2025-02-08T13:07:59.193723Z",
     "shell.execute_reply": "2025-02-08T13:07:59.192655Z",
     "shell.execute_reply.started": "2025-02-08T13:07:59.047300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdownloads\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:08:06.533629Z",
     "iopub.status.busy": "2025-02-08T13:08:06.533297Z",
     "iopub.status.idle": "2025-02-08T13:08:06.541537Z",
     "shell.execute_reply": "2025-02-08T13:08:06.540692Z",
     "shell.execute_reply.started": "2025-02-08T13:08:06.533599Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/downloads\n"
     ]
    }
   ],
   "source": [
    "cd downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3️⃣ **Downloading Video from YouTube**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:08:13.862443Z",
     "iopub.status.busy": "2025-02-08T13:08:13.862149Z",
     "iopub.status.idle": "2025-02-08T13:08:13.867116Z",
     "shell.execute_reply": "2025-02-08T13:08:13.866249Z",
     "shell.execute_reply.started": "2025-02-08T13:08:13.862414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def download_youtube_video(youtube_url, download_path):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo+bestaudio/best',\n",
    "        'outtmpl': f\"{download_path}/%(title)s.%(ext)s\"\n",
    "    }\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([youtube_url])\n",
    "        print(f\"✅ Video downloaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:08:16.011812Z",
     "iopub.status.busy": "2025-02-08T13:08:16.011534Z",
     "iopub.status.idle": "2025-02-08T13:08:16.015661Z",
     "shell.execute_reply": "2025-02-08T13:08:16.014757Z",
     "shell.execute_reply.started": "2025-02-08T13:08:16.011789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def download_youtube_video(youtube_url, download_path):\n",
    "#     os.makedirs(download_path, exist_ok=True)\n",
    "    \n",
    "#     command = f'yt-dlp -f \"bestvideo[ext=mp4]+bestaudio[ext=m4a]\" --merge-output-format mp4 -o \"{download_path}/%(title)s.%(ext)s\" \"{youtube_url}\"'\n",
    "    \n",
    "#     os.system(command)\n",
    "#     print(\"✅ Downloaded and merged as MP4!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:08:25.007568Z",
     "iopub.status.busy": "2025-02-08T13:08:25.007285Z",
     "iopub.status.idle": "2025-02-08T13:08:30.217155Z",
     "shell.execute_reply": "2025-02-08T13:08:30.216478Z",
     "shell.execute_reply.started": "2025-02-08T13:08:25.007548Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter YouTube video link:  https://youtu.be/NiKtZgImdlY?feature=shared\n"
     ]
    }
   ],
   "source": [
    "youtube_link = input(\"Enter YouTube video link: \")\n",
    "# save_path = input(\"Enter folder path to save video: \")\n",
    "savePath = r\"/kaggle/working/downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:08:46.434775Z",
     "iopub.status.busy": "2025-02-08T13:08:46.434478Z",
     "iopub.status.idle": "2025-02-08T13:08:52.306473Z",
     "shell.execute_reply": "2025-02-08T13:08:52.305779Z",
     "shell.execute_reply.started": "2025-02-08T13:08:46.434751Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/NiKtZgImdlY?feature=shared\n",
      "[youtube] NiKtZgImdlY: Downloading webpage\n",
      "[youtube] NiKtZgImdlY: Downloading tv client config\n",
      "[youtube] NiKtZgImdlY: Downloading player 9c6dfc4a\n",
      "[youtube] NiKtZgImdlY: Downloading tv player API JSON\n",
      "[youtube] NiKtZgImdlY: Downloading ios player API JSON\n",
      "[youtube] NiKtZgImdlY: Downloading m3u8 information\n",
      "[info] NiKtZgImdlY: Downloading 1 format(s): 398+251\n",
      "[download] Destination: /kaggle/working/downloads/The danger of silence ｜ Clint Smith ｜ TED.f398.mp4\n",
      "[download] 100% of   11.02MiB in 00:00:00 at 22.90MiB/s    \n",
      "[download] Destination: /kaggle/working/downloads/The danger of silence ｜ Clint Smith ｜ TED.f251.webm\n",
      "[download] 100% of    3.41MiB in 00:00:00 at 5.31MiB/s   \n",
      "[Merger] Merging formats into \"/kaggle/working/downloads/The danger of silence ｜ Clint Smith ｜ TED.webm\"\n",
      "Deleting original file /kaggle/working/downloads/The danger of silence ｜ Clint Smith ｜ TED.f398.mp4 (pass -k to keep)\n",
      "Deleting original file /kaggle/working/downloads/The danger of silence ｜ Clint Smith ｜ TED.f251.webm (pass -k to keep)\n",
      "✅ Video downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "download_youtube_video(youtube_link, savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Conversion to .mp4 Format*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"yt-dlp\" downloads video in '.webm' format. So it needs to be converted to '.mp4' for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:09:57.808191Z",
     "iopub.status.busy": "2025-02-08T13:09:57.807863Z",
     "iopub.status.idle": "2025-02-08T13:11:33.835668Z",
     "shell.execute_reply": "2025-02-08T13:11:33.834753Z",
     "shell.execute_reply.started": "2025-02-08T13:09:57.808162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted to MP4: /kaggle/working/downloads/The danger of silence ｜ Clint Smith ｜ TED.mp4\n"
     ]
    }
   ],
   "source": [
    "def convert_webm_to_mp4(input_path):\n",
    "    output_path = input_path.replace(\".webm\", \".mp4\")\n",
    "    os.system(f\"ffmpeg -i \\\"{input_path}\\\" -c:v libx264 -preset fast -crf 23 -c:a aac -b:a 192k \\\"{output_path}\\\"\")\n",
    "    print(f\"✅ Converted to MP4: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "webm_file = r\"/kaggle/working/downloads/The danger of silence ｜ Clint Smith ｜ TED.webm\"\n",
    "mp4_file = convert_webm_to_mp4(webm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4️⃣ **Setup the Video Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:12:00.768428Z",
     "iopub.status.busy": "2025-02-08T13:12:00.768008Z",
     "iopub.status.idle": "2025-02-08T13:12:00.772694Z",
     "shell.execute_reply": "2025-02-08T13:12:00.771663Z",
     "shell.execute_reply.started": "2025-02-08T13:12:00.768395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# yt = r\"/kaggle/input/tedx-video/woody-roseland--tedxmilehigh---a-one-minute-tedx-talk-for-the-digital-age.mp4\"\n",
    "# yt = r\"/kaggle/working/downloads/＂Mastering Happiness： The Watermelon Lesson＂.mp4\"\n",
    "yt = r\"/kaggle/working/downloads/The danger of silence ｜ Clint Smith ｜ TED.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:12:02.307656Z",
     "iopub.status.busy": "2025-02-08T13:12:02.307350Z",
     "iopub.status.idle": "2025-02-08T13:12:02.312485Z",
     "shell.execute_reply": "2025-02-08T13:12:02.311546Z",
     "shell.execute_reply.started": "2025-02-08T13:12:02.307635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(os.path.exists(yt))  # Should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:12:24.896371Z",
     "iopub.status.busy": "2025-02-08T13:12:24.896068Z",
     "iopub.status.idle": "2025-02-08T13:12:24.901843Z",
     "shell.execute_reply": "2025-02-08T13:12:24.901059Z",
     "shell.execute_reply.started": "2025-02-08T13:12:24.896349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/downloads'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5️⃣ **Extracting Audio from Video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:13:12.363329Z",
     "iopub.status.busy": "2025-02-08T13:13:12.362930Z",
     "iopub.status.idle": "2025-02-08T13:13:12.367111Z",
     "shell.execute_reply": "2025-02-08T13:13:12.366197Z",
     "shell.execute_reply.started": "2025-02-08T13:13:12.363299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "outputAudio = r\"/kaggle/working/downloads/silence.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:13:13.871632Z",
     "iopub.status.busy": "2025-02-08T13:13:13.871342Z",
     "iopub.status.idle": "2025-02-08T13:13:14.421343Z",
     "shell.execute_reply": "2025-02-08T13:13:14.420594Z",
     "shell.execute_reply.started": "2025-02-08T13:13:13.871609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Sagar! Audio extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "ffmpeg.input(yt).output(outputAudio, format=\"wav\").run(overwrite_output=True)\n",
    "print(\"Hey Sagar! Audio extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T07:32:34.664133Z",
     "iopub.status.busy": "2025-02-08T07:32:34.663797Z",
     "iopub.status.idle": "2025-02-08T07:32:34.667882Z",
     "shell.execute_reply": "2025-02-08T07:32:34.666878Z",
     "shell.execute_reply.started": "2025-02-08T07:32:34.664109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ! ffmpeg -i /kaggle/input/tedx-video/woody-roseland--tedxmilehigh---a-one-minute-tedx-talk-for-the-digital-age.mp4 -acodec pcm_s16le -ar 16000 ytAudio.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6️⃣ **English ASR with HuggingSound🎵🔊**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:14:11.808714Z",
     "iopub.status.busy": "2025-02-08T13:14:11.808396Z",
     "iopub.status.idle": "2025-02-08T13:14:15.339034Z",
     "shell.execute_reply": "2025-02-08T13:14:15.337822Z",
     "shell.execute_reply.started": "2025-02-08T13:14:11.808680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install huggingsound -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:14:19.483199Z",
     "iopub.status.busy": "2025-02-08T13:14:19.482835Z",
     "iopub.status.idle": "2025-02-08T13:14:24.456900Z",
     "shell.execute_reply": "2025-02-08T13:14:24.456039Z",
     "shell.execute_reply.started": "2025-02-08T13:14:19.483166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers loaded successfully!\n",
      "Huggingsound loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "print(\"Transformers loaded successfully!\")\n",
    "from huggingsound import SpeechRecognitionModel #Upgrade CUDA, if error occurs\n",
    "print(\"Huggingsound loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:14:26.677959Z",
     "iopub.status.busy": "2025-02-08T13:14:26.677262Z",
     "iopub.status.idle": "2025-02-08T13:14:26.751919Z",
     "shell.execute_reply": "2025-02-08T13:14:26.751173Z",
     "shell.execute_reply.started": "2025-02-08T13:14:26.677925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the device to CUDA (for GPU-enabled environments)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Login to Hugging Face with Access Token*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:14:29.756727Z",
     "iopub.status.busy": "2025-02-08T13:14:29.756450Z",
     "iopub.status.idle": "2025-02-08T13:14:29.773893Z",
     "shell.execute_reply": "2025-02-08T13:14:29.772951Z",
     "shell.execute_reply.started": "2025-02-08T13:14:29.756706Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed99ad317244b73b95b952b6a15564d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7️⃣ **Load the Model from 🤗/Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:14:43.212119Z",
     "iopub.status.busy": "2025-02-08T13:14:43.211799Z",
     "iopub.status.idle": "2025-02-08T13:14:54.036907Z",
     "shell.execute_reply": "2025-02-08T13:14:54.036253Z",
     "shell.execute_reply.started": "2025-02-08T13:14:43.212096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e48672c96e46dd85a6e2c3ca0b892c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49383ef73a4e4597b16bc78212770aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94188c21dec047799223a38dce8a702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/262 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b90d8519704730976eb9926f8db1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/300 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd306fb9fbc54ac4ab60f5c94227caea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\", device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8️⃣ **Audio Chunking (To Avoid Out of Memory Error)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:15:16.670874Z",
     "iopub.status.busy": "2025-02-08T13:15:16.670583Z",
     "iopub.status.idle": "2025-02-08T13:15:20.494909Z",
     "shell.execute_reply": "2025-02-08T13:15:20.493985Z",
     "shell.execute_reply.started": "2025-02-08T13:15:16.670853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:15:22.088712Z",
     "iopub.status.busy": "2025-02-08T13:15:22.088390Z",
     "iopub.status.idle": "2025-02-08T13:15:22.129842Z",
     "shell.execute_reply": "2025-02-08T13:15:22.129175Z",
     "shell.execute_reply.started": "2025-02-08T13:15:22.088686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:15:30.244370Z",
     "iopub.status.busy": "2025-02-08T13:15:30.243939Z",
     "iopub.status.idle": "2025-02-08T13:15:30.249393Z",
     "shell.execute_reply": "2025-02-08T13:15:30.248414Z",
     "shell.execute_reply.started": "2025-02-08T13:15:30.244340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_audio(audio_path, min_silence_len=1000, silence_thresh=-40, chunk_length_ms=30000):\n",
    "    \"\"\"\n",
    "    Splits audio into smaller chunks based on silence detection or fixed intervals.\n",
    "\n",
    "    Parameters:\n",
    "        - audio_path (str): Path to input audio file.\n",
    "        - min_silence_len (int): Minimum silence length (ms) to consider as a split.\n",
    "        - silence_thresh (int): Silence threshold (dBFS) for detecting splits.\n",
    "        - chunk_length_ms (int): Maximum chunk length in milliseconds.\n",
    "\n",
    "    Returns:\n",
    "        - List of AudioSegment chunks.\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    \n",
    "    # First, try splitting based on silence\n",
    "    chunks = split_on_silence(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
    "\n",
    "    # If silence-based splitting is not effective, split into fixed chunks\n",
    "    if len(chunks) == 1:  \n",
    "        chunks = [audio[i : i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:15:38.344158Z",
     "iopub.status.busy": "2025-02-08T13:15:38.343848Z",
     "iopub.status.idle": "2025-02-08T13:15:38.349704Z",
     "shell.execute_reply": "2025-02-08T13:15:38.349009Z",
     "shell.execute_reply.started": "2025-02-08T13:15:38.344134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:15:42.870414Z",
     "iopub.status.busy": "2025-02-08T13:15:42.870130Z",
     "iopub.status.idle": "2025-02-08T13:15:43.022477Z",
     "shell.execute_reply": "2025-02-08T13:15:43.021587Z",
     "shell.execute_reply.started": "2025-02-08T13:15:42.870387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdownloads\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:15:50.868525Z",
     "iopub.status.busy": "2025-02-08T13:15:50.868196Z",
     "iopub.status.idle": "2025-02-08T13:15:51.019685Z",
     "shell.execute_reply": "2025-02-08T13:15:51.018511Z",
     "shell.execute_reply.started": "2025-02-08T13:15:50.868493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mkdir audioChunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:15:55.661574Z",
     "iopub.status.busy": "2025-02-08T13:15:55.661261Z",
     "iopub.status.idle": "2025-02-08T13:15:55.811996Z",
     "shell.execute_reply": "2025-02-08T13:15:55.810980Z",
     "shell.execute_reply.started": "2025-02-08T13:15:55.661549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34maudioChunks\u001b[0m/  \u001b[01;34mdownloads\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:16:33.767182Z",
     "iopub.status.busy": "2025-02-08T13:16:33.766785Z",
     "iopub.status.idle": "2025-02-08T13:16:33.771616Z",
     "shell.execute_reply": "2025-02-08T13:16:33.770827Z",
     "shell.execute_reply.started": "2025-02-08T13:16:33.767150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# audio = r\"/kaggle/working/ytAudio.wav\"\n",
    "audio = r\"/kaggle/working/downloads/silence.wav\"\n",
    "# output_dir = r\"/kaggle/working/audioChunks\"\n",
    "output_dir = r\"/kaggle/working/audioChunks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:16:36.577098Z",
     "iopub.status.busy": "2025-02-08T13:16:36.576796Z",
     "iopub.status.idle": "2025-02-08T13:17:12.130205Z",
     "shell.execute_reply": "2025-02-08T13:17:12.129203Z",
     "shell.execute_reply.started": "2025-02-08T13:16:36.577075Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 39\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "chunks = split_audio(audio)\n",
    "\n",
    "chunk_paths = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_filename = f\"{output_dir}/chunk_{i}.wav\"\n",
    "    chunk.export(chunk_filename, format=\"wav\")\n",
    "    chunk_paths.append(chunk_filename)\n",
    "\n",
    "print(f\"Total Chunks: {len(chunk_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:17:53.680548Z",
     "iopub.status.busy": "2025-02-08T13:17:53.680227Z",
     "iopub.status.idle": "2025-02-08T13:17:53.686255Z",
     "shell.execute_reply": "2025-02-08T13:17:53.685494Z",
     "shell.execute_reply.started": "2025-02-08T13:17:53.680520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/audioChunks\n"
     ]
    }
   ],
   "source": [
    "cd audioChunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:17:55.909033Z",
     "iopub.status.busy": "2025-02-08T13:17:55.908738Z",
     "iopub.status.idle": "2025-02-08T13:17:56.059886Z",
     "shell.execute_reply": "2025-02-08T13:17:56.059076Z",
     "shell.execute_reply.started": "2025-02-08T13:17:55.908995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_0.wav   chunk_17.wav  chunk_24.wav  chunk_31.wav  chunk_3.wav\n",
      "chunk_10.wav  chunk_18.wav  chunk_25.wav  chunk_32.wav  chunk_4.wav\n",
      "chunk_11.wav  chunk_19.wav  chunk_26.wav  chunk_33.wav  chunk_5.wav\n",
      "chunk_12.wav  chunk_1.wav   chunk_27.wav  chunk_34.wav  chunk_6.wav\n",
      "chunk_13.wav  chunk_20.wav  chunk_28.wav  chunk_35.wav  chunk_7.wav\n",
      "chunk_14.wav  chunk_21.wav  chunk_29.wav  chunk_36.wav  chunk_8.wav\n",
      "chunk_15.wav  chunk_22.wav  chunk_2.wav   chunk_37.wav  chunk_9.wav\n",
      "chunk_16.wav  chunk_23.wav  chunk_30.wav  chunk_38.wav\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:17:57.894960Z",
     "iopub.status.busy": "2025-02-08T13:17:57.894604Z",
     "iopub.status.idle": "2025-02-08T13:17:57.900686Z",
     "shell.execute_reply": "2025-02-08T13:17:57.899983Z",
     "shell.execute_reply.started": "2025-02-08T13:17:57.894927Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:18:31.887370Z",
     "iopub.status.busy": "2025-02-08T13:18:31.887080Z",
     "iopub.status.idle": "2025-02-08T13:18:31.890854Z",
     "shell.execute_reply": "2025-02-08T13:18:31.889972Z",
     "shell.execute_reply.started": "2025-02-08T13:18:31.887350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chunksFolder = r\"/kaggle/working/audioChunks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9️⃣ **Transcribing Each Audio Chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:19:41.634624Z",
     "iopub.status.busy": "2025-02-08T13:19:41.634304Z",
     "iopub.status.idle": "2025-02-08T13:19:41.638840Z",
     "shell.execute_reply": "2025-02-08T13:19:41.638175Z",
     "shell.execute_reply.started": "2025-02-08T13:19:41.634598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List all audio chunk files\n",
    "chunk_files = sorted([os.path.join(chunksFolder, f) for f in os.listdir(chunksFolder) if f.endswith(\".wav\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:19:43.666537Z",
     "iopub.status.busy": "2025-02-08T13:19:43.666256Z",
     "iopub.status.idle": "2025-02-08T13:19:54.707968Z",
     "shell.execute_reply": "2025-02-08T13:19:54.707104Z",
     "shell.execute_reply.started": "2025-02-08T13:19:43.666516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.89it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Transcribe each chunk\n",
    "transcripts = []\n",
    "for chunk in chunk_files:\n",
    "    result = model.transcribe([chunk])\n",
    "    transcripts.append(result[0][\"transcription\"])  # Extract text from result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Combine all the transcripts (of each chunk) into Single Transcript*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:20:05.137689Z",
     "iopub.status.busy": "2025-02-08T13:20:05.137390Z",
     "iopub.status.idle": "2025-02-08T13:20:05.142562Z",
     "shell.execute_reply": "2025-02-08T13:20:05.141714Z",
     "shell.execute_reply.started": "2025-02-08T13:20:05.137658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Full Transcript:\n",
      "\n",
      "oepepeps apear doctor martin luther king junior in a nineteen-sixty-eighth beach where he reflects upon the civil rights movt state ename to understand that they don't have to be sources of shame in an effort to create a culture within my classroom where students feel safe sharing te intimacies of their own silence i have four corp principles posted on the board that sits in the front of my clas every student signs the beginning of the year read critically right-consciously speak clearly tell youre i fiund myself thinking a lot about that last tell youre and i realize that if i was going to ask my students to speak up i' was going tn have to tell mice in the end and be honest with them about times why i fail to do so i tell them that growing up as a kid in a catholic family in new orlens i was always tought that the most meaningful thing one could do was to give something upsacrificed somthing youtypically indulgent to prove to god you understand his sanctity i've given up sodo mcdonald's french prize french kisses and everything but one year i gave up speak figure the most valuable thing i could sacrifice was my own voice but it was like i haden' realise that i had given that up a long time i have spent so much of my life telling people the things they wanted to hear instead of the things they needed to told myself i wasn't meant to be anyone's consciense because i still had to figure out being my own so sometimes i just wouldn't say anythingappeasing ignorance with my silence unaware that validn doesn't ned words to endorse its existence when christian was beat up for being gay i put my hands in my pocket and walk with y head down as if i didn't even notice couldn't use my locker for weeks because bot on he lock reminded me of the one i put on my lips when te homeless man on corner looked at me with eyes up mearly searching for an afformation that he was worth seeing i was more concern withtouching the screen on my apple than actually feeding him oe when the woman at hefun raising galler sand so proud of you it must be so hard teaching those poor unintelligent kids i bit my lip because apparently weneeded her money ore my students needed their dignity we spend so much time listening to the things people are saying we will remember not the words of our enemie that we rarely pay attention to the things they don't silent s the residue of fearit is feeling your flaws gutridge gillotine your tongue it is the hair retreating from your chest because it doesn't feel safe in your lungs silence is want in genocide silence its catrina tis what you hear when their arnt enough bodybags left it is the sound after the noose is already tred it is charring it s chain t is privilege itis pain there is no time to pick your battles when your battles have already picked you i will not let silence wrap itself around my indecision i will tell christian that he is a lion a sanctuary of bravery and brilliant i will asked that homeless man what his name is and how his day was because sometimes all people want to be is human i will tell that woman the  students can talk about transdenalism like their last name astrow and just because you watch one episode wire doesn't you know anything about mahid so this year nstead of giving something up i will live every day as if there were a microphone tucked under my tongue a stage on the underside of my inhibbitin because who has to ever soap bas whernow you've ever needed he boy ee eeeeeiiaa ta eteves but the silence of our fried as a teacher i've internalized this miss every day all around us we see the consequences of silence manifest themselves in the form of discrimination violence genocide and war in the classroom i challenge my students to explore the silences in their own lives through poet we work together to fill those space o recking nise\n"
     ]
    }
   ],
   "source": [
    "full_transcript = \" \".join(transcripts)\n",
    "print(\"\\n✅ Full Transcript:\\n\")\n",
    "print(full_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣0️⃣ **Transcribe Translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:20:17.482095Z",
     "iopub.status.busy": "2025-02-08T13:20:17.481773Z",
     "iopub.status.idle": "2025-02-08T13:20:17.487631Z",
     "shell.execute_reply": "2025-02-08T13:20:17.486592Z",
     "shell.execute_reply.started": "2025-02-08T13:20:17.482071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Would you like to translate the transcript? Available languages: \n",
      "hi -> Hindi\n",
      "es -> Spanish\n",
      "fr -> French\n",
      "de -> German\n",
      "zh -> Chinese\n",
      "None -> No Translation\n"
     ]
    }
   ],
   "source": [
    "# List of available languages\n",
    "languages = {\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"fr\": \"French\",\n",
    "    \"de\": \"German\",\n",
    "    \"zh\": \"Chinese\",\n",
    "    \"None\": \"No Translation\"\n",
    "}\n",
    "\n",
    "print(\"\\nWould you like to translate the transcript? Available languages: \")\n",
    "for code, lang in languages.items():\n",
    "    print(f\"{code} -> {lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:20:20.838832Z",
     "iopub.status.busy": "2025-02-08T13:20:20.838548Z",
     "iopub.status.idle": "2025-02-08T13:20:22.947992Z",
     "shell.execute_reply": "2025-02-08T13:20:22.947317Z",
     "shell.execute_reply.started": "2025-02-08T13:20:20.838809Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter language code (or 'None' to skip):  hi\n"
     ]
    }
   ],
   "source": [
    "selected_lang = input(\"\\nEnter language code (or 'None' to skip): \").strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:20:25.596151Z",
     "iopub.status.busy": "2025-02-08T13:20:25.595566Z",
     "iopub.status.idle": "2025-02-08T13:20:26.982847Z",
     "shell.execute_reply": "2025-02-08T13:20:26.982001Z",
     "shell.execute_reply.started": "2025-02-08T13:20:25.596100Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Translated Transcript (Hindi):\n",
      "\n",
      "oepepeps apear डॉक्टर मार्टिन लूथर किंग जूनियर एक उन्नीस-छह-आठवें समुद्र तट में जहां वह नागरिक अधिकारों Movt राज्य को यह समझने के लिए प्रतिबिंबित करता है कि उन्हें मेरी कक्षा के भीतर एक संस्कृति बनाने के प्रयास में शर्म के स्रोत नहीं हैं, जहां छात्र छात्र हैं अपने स्वयं के चुप्पी की सुरक्षित साझा करने के लिए सुरक्षित साझा करना मेरे पास बोर्ड पर चार कॉर्प सिद्धांत हैं जो मेरे सीएलएएस के सामने बैठते हैं, यह आखिरी बार आपको बताता है और मुझे एहसास है कि अगर मैं अपने छात्रों को बोलने के लिए कहने जा रहा था तो मैं अंत में चूहों को बता रहा था और उनके साथ ईमानदार होना चाहिए कि मैं ऐसा करने में विफल क्यों हूं इसलिए मैं उन्हें बताता हूं कि बड़े हो रहे हैं। न्यू ऑरलेंस में एक कैथोलिक परिवार में एक बच्चे के रूप में, मुझे हमेशा लगता था कि सबसे सार्थक बात यह थी सब कुछ, लेकिन एक साल मैंने छोड़ दिया, सबसे मूल्यवान चीज जो मैं बलिदान कर सकता था वह मेरी खुद की आवाज थी, लेकिन यह ऐसा था जैसे मैंने हेडन को एहसास किया कि मैंने दिया था कि मैंने लंबे समय तक अपने जीवन का इतना खर्च किया है। उन चीजों के बजाय सुनना चाहता था जो उन्हें खुद को बताने की ज़रूरत थी कि मैं किसी के विवेक के लिए नहीं था क्योंकि मुझे अभी भी अपना खुद का पता लगाना था, इसलिए कभी -कभी मैं सिर्फ अपने मौन के साथ अज्ञानता से कुछ भी नहीं कहूंगा कि वैध नहीं है। नेड शब्द अपने अस्तित्व का समर्थन करने के लिए जब ईसाई को समलैंगिक होने के लिए मार दिया गया था, मैंने अपने हाथों को अपनी जेब में डाल दिया और वाई हेड के साथ चलो जैसे कि मैंने नोटिस भी नहीं किया था कि मैं अपने लॉकर का उपयोग हफ्तों तक नहीं कर सकता था क्योंकि बॉट ने मुझे याद दिलाया कि वह मुझे याद दिलाता है एक मैं अपने होंठों पर डाल दिया जब कोने पर ते बेघर आदमी ने मुझे आँखों से देखा कि वह एक सेंसिंग की खोज कर रहा था कि वह यह देखने लायक था गैलर रेत को उठाते हुए आप पर इतना गर्व है कि यह उन गरीब अनजाने बच्चों को पढ़ाने में बहुत कठिन होना चाहिए जो मैंने अपने होंठ को थोड़ा सा पढ़ा क्योंकि जाहिरा तौर पर उसके पैसे अयस्क को अयस्क किया जाता है, मेरे छात्रों को उनकी गरिमा की जरूरत थी, हम उन चीजों को सुनने में बहुत समय बिताते हैं जो लोग कह रहे हैं कि हम शब्दों को याद नहीं करेंगे कि हम शब्दों को याद नहीं करेंगे हमारे एनीमी के बारे में कि हम शायद ही कभी उन चीजों पर ध्यान देते हैं जो वे चुप नहीं करते हैं कि डर का अवशेष आपकी खामियों को महसूस कर रहा है कि गुट्रिज गिलोटीन आपकी जीभ यह है कि यह आपके सीने से पीछे हटने वाला बाल है क्योंकि यह आपके फेफड़ों में सुरक्षित नहीं है नरसंहार की चुप्पी में इसकी कैटरीना टिस क्या आप सुनते हैं जब उनके arnt पर्याप्त बॉडीबैग छोड़े गए हैं, यह ध्वनि है क्योंकि नोज पहले से ही ट्रेड है यह चेन टी टी है टी टी का विशेषाधिकार है यह दर्द है कि आपकी लड़ाइयों को लेने के लिए कोई समय नहीं है जब आपकी लड़ाई पहले से ही उठा चुकी है। आप अपने अनिर्णय के चारों ओर अपने आप को चुप्पी नहीं लाने देंगे, मैं क्रिश्चियन को बताऊंगा कि वह एक शेर है जो बहादुरी का एक अभयारण्य है और शानदार मैं पूछूंगा कि बेघर आदमी उसका नाम क्या है और उसका दिन कैसा था क्योंकि कभी -कभी सभी लोग मानवीय होना चाहते हैं मैं बताऊंगा कि महिला अपने अंतिम नाम एस्ट्रो की तरह ट्रांसडेनलिज़्म के बारे में बात कर सकती है और सिर्फ इसलिए कि आप एक एपिसोड वायर देखते हैं, आप महिद के बारे में कुछ भी नहीं जानते हैं, इसलिए इस साल कुछ देने के लिए मैं हर दिन जीऊंगा जैसे कि कोई भी था माइक्रोफोन मेरी जीभ के नीचे मेरे इनहिबिटिन के नीचे एक मंच के नीचे टक गया क्योंकि जो कभी भी साबुन के लिए होता है हमारे आसपास हम देखते हैं कि मौन के परिणाम खुद को भेदभाव हिंसा नरसंहार के रूप में प्रकट करते हैं और कक्षा में युद्ध मैं अपने छात्रों को चुनौती देता हूं कि वे अपने स्वयं के जीवन में चुप्पी का पता लगाने के लिए कवि के माध्यम से हम उन स्थानों को भरने के लिए एक साथ काम करें,\n"
     ]
    }
   ],
   "source": [
    "if selected_lang in languages and selected_lang != \"None\":\n",
    "    translated_text = GoogleTranslator(source=\"auto\", target=selected_lang).translate(full_transcript)\n",
    "    print(f\"\\n✅ Translated Transcript ({languages[selected_lang]}):\\n\")\n",
    "    print(translated_text)\n",
    "else:\n",
    "    translated_text = full_transcript  \n",
    "    print(\"\\n✅ Skipping translation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣1️⃣ **Transcript Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model from 🤗/Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:20:40.864962Z",
     "iopub.status.busy": "2025-02-08T13:20:40.864659Z",
     "iopub.status.idle": "2025-02-08T13:20:51.808521Z",
     "shell.execute_reply": "2025-02-08T13:20:51.807564Z",
     "shell.execute_reply.started": "2025-02-08T13:20:40.864939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a11c359b7d8499aac53ffad2e90dbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7d64c079d54117b0a030402c808a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2f9e9422ef4bad8cecc9282e469a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a551ae0c86f744e8b0ff1d466186a000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a7281d6ff746dca92b9bfe84c84e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f19be1874044d297cf88a582358d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44037329d067431689c34f010748968f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:21:14.085910Z",
     "iopub.status.busy": "2025-02-08T13:21:14.085611Z",
     "iopub.status.idle": "2025-02-08T13:21:18.155143Z",
     "shell.execute_reply": "2025-02-08T13:21:18.154414Z",
     "shell.execute_reply.started": "2025-02-08T13:21:14.085887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "summary = summarizer(full_transcript, max_length=500, min_length=300, do_sample=False)[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:21:23.887068Z",
     "iopub.status.busy": "2025-02-08T13:21:23.886745Z",
     "iopub.status.idle": "2025-02-08T13:21:23.891256Z",
     "shell.execute_reply": "2025-02-08T13:21:23.890328Z",
     "shell.execute_reply.started": "2025-02-08T13:21:23.887009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Summary of the Video:\n",
      "\n",
      "oepepeps apear doctor martin luther king junior in a nineteen-sixty-eighth beach where he reflects upon the civil rights movt state ename to understand that they don't have to be sources of shame in an effort to create a culture within my classroom . i've internalized this miss every day all around us we see the consequences of silence manifest themselves in the form of discrimination violence genocide and war... i have four corp principles posted on the board that 's - aps to tell christian that he was tn have to a 'n' eeps on the 'ename' to read critically right-consciously speak clearly tell youre - and he's the most . the . students feel . they . that . in . our . we see . as a teacher . it is . and i’ . of . their . (i .. . many . ‘sacrifice . de . my . I . have . this . but . to . how . le . on . what if . be ., i will . for a while .\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n✅ Summary of the Video:\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣2️⃣ **Q&A Chatbot🤖 (Context: YouTube Transcript)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:21:56.098309Z",
     "iopub.status.busy": "2025-02-08T13:21:56.097877Z",
     "iopub.status.idle": "2025-02-08T13:21:56.355324Z",
     "shell.execute_reply": "2025-02-08T13:21:56.354683Z",
     "shell.execute_reply.started": "2025-02-08T13:21:56.098283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load any Sentence-Transformer Model for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:22:03.806720Z",
     "iopub.status.busy": "2025-02-08T13:22:03.806413Z",
     "iopub.status.idle": "2025-02-08T13:22:11.329810Z",
     "shell.execute_reply": "2025-02-08T13:22:11.328861Z",
     "shell.execute_reply.started": "2025-02-08T13:22:03.806695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe1e805dfdb4d3cb4c3bd278c1de6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22aa2deb67548688e901fbef88d521a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d9d49a46cb4058a2da26949a87f816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eaf6aabdd22418e862740b4db12272c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a8c26ff238427da1461cce2c4cd120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb46e2d46dc64aedadea9568256eb602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f977c7777b84dffaddce0e40a81a5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d449d79244ae45d687221946395d903c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5ed9710af34a428efffa368c7b1cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9040eec50be84f9fbf7fdcf199310c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305032e707154a69aa08967fc67191e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣3️⃣ **Create FAISS Index to store mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:24:42.697494Z",
     "iopub.status.busy": "2025-02-08T13:24:42.697141Z",
     "iopub.status.idle": "2025-02-08T13:24:42.702356Z",
     "shell.execute_reply": "2025-02-08T13:24:42.701361Z",
     "shell.execute_reply.started": "2025-02-08T13:24:42.697469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_faiss_index(transcript_chunks):\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from transcript chunks and stores mappings.\n",
    "    \"\"\"\n",
    "    embeddings = embedding_model.encode(transcript_chunks)\n",
    "    dimension = embeddings.shape[1]\n",
    "\n",
    "    # Initialize FAISS index\n",
    "    index = faiss.IndexFlatL2(dimension)  \n",
    "    index.add(np.array(embeddings, dtype=np.float32))  # Convert to float32\n",
    "\n",
    "    # Store transcript chunks in a dictionary for easy retrieval\n",
    "    transcript_dict = {i: transcript_chunks[i] for i in range(len(transcript_chunks))}\n",
    "\n",
    "    return index, transcript_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model for Q&A from 🤗/Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:26:37.280824Z",
     "iopub.status.busy": "2025-02-08T13:26:37.280534Z",
     "iopub.status.idle": "2025-02-08T13:26:48.137101Z",
     "shell.execute_reply": "2025-02-08T13:26:48.136147Z",
     "shell.execute_reply.started": "2025-02-08T13:26:37.280801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18568c61e0e4685a8cc3ce3d875c12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fd8b4549af47449a2a3b1ca7ce5e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d761d742d02f4d9998a55d1bc3974439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9492bd680bd458fa95563f251432eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6bd9e6fe3248d78c104d1f9aef9270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f4798fc4154dfbbbb10e0ba9467cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dfb57b7c644429a685ace12c14325f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "qa_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:27:08.399332Z",
     "iopub.status.busy": "2025-02-08T13:27:08.399013Z",
     "iopub.status.idle": "2025-02-08T13:27:08.557975Z",
     "shell.execute_reply": "2025-02-08T13:27:08.557302Z",
     "shell.execute_reply.started": "2025-02-08T13:27:08.399310Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24a6a1dbc874b0e813fa430ec3943f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index, transcript_dict = create_faiss_index(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:28:02.961299Z",
     "iopub.status.busy": "2025-02-08T13:28:02.960939Z",
     "iopub.status.idle": "2025-02-08T13:28:02.965941Z",
     "shell.execute_reply": "2025-02-08T13:28:02.965051Z",
     "shell.execute_reply.started": "2025-02-08T13:28:02.961274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_relevant_text(query, index, transcript_dict, top_k=2):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant transcript chunk(s) using FAISS.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    _, indices = index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "    \n",
    "    retrieved_texts = [transcript_dict[idx] for idx in indices[0]]\n",
    "    return \" \".join(retrieved_texts)  # Merge top-k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:28:16.374906Z",
     "iopub.status.busy": "2025-02-08T13:28:16.374611Z",
     "iopub.status.idle": "2025-02-08T13:28:16.379679Z",
     "shell.execute_reply": "2025-02-08T13:28:16.378814Z",
     "shell.execute_reply.started": "2025-02-08T13:28:16.374884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_answer(question, context):\n",
    "    \"\"\"\n",
    "    Uses FLAN-T5 to generate an answer based on the retrieved context.\n",
    "    \"\"\"\n",
    "    input_text = f\"Context: {context} \\n Question: {question}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = qa_model.generate(**inputs, max_length=100)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣4️⃣ **Setup for the ChatBot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:28:43.878665Z",
     "iopub.status.busy": "2025-02-08T13:28:43.878351Z",
     "iopub.status.idle": "2025-02-08T13:28:43.883681Z",
     "shell.execute_reply": "2025-02-08T13:28:43.882616Z",
     "shell.execute_reply.started": "2025-02-08T13:28:43.878641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def chat_with_video_qa(index, transcript_dict):\n",
    "    \"\"\"\n",
    "    Interactive chatbot that answers questions using FAISS + FLAN-T5.\n",
    "    \"\"\"\n",
    "    print(\"💬 Ask anything from the video! (Type 'exit' to stop)\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"🧐 You: \")\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"👋 Exiting chatbot. Have a great day!\")\n",
    "            break\n",
    "\n",
    "        # Retrieve relevant transcript chunks\n",
    "        context = get_relevant_text(question, index, transcript_dict, top_k=2)\n",
    "\n",
    "        # Generate an answer\n",
    "        answer = generate_answer(question, context)\n",
    "        print(f\"🤖 AI: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣5️⃣ **Start the Chat🚀**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:28:55.860956Z",
     "iopub.status.busy": "2025-02-08T13:28:55.860660Z",
     "iopub.status.idle": "2025-02-08T13:30:22.101610Z",
     "shell.execute_reply": "2025-02-08T13:30:22.100866Z",
     "shell.execute_reply.started": "2025-02-08T13:28:55.860933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Ask anything from the video! (Type 'exit' to stop)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧐 You:  What is this video about?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2715a7b19a417dad174c2c7c29f95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 AI: Science/Tech\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧐 You:  What is the key takeaway from this video?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970386fba9aa4eada76bb72f5b984075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 AI: The video shows the people who are not paying attention to the things they don't.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧐 You:  What should we learn from this video?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0aa4ef80c94c1bbe720e9435ce1b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 AI: The people in the space are not able to pay attention to the things they don't work together to fill those spaces\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧐 You:  Would you like to add some more information, to this video?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51263da27cb4413d9dbfee532b784e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 AI: no\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧐 You:  Thanks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d46fb7aea44494cbdb880144f2683d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 AI: i'm sorry\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧐 You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Exiting chatbot. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "chat_with_video_qa(index, transcript_dict)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
